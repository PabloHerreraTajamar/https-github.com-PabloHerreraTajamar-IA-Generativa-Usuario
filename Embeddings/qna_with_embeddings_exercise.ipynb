{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI Service - Q&A with semantic answering exerise\n",
    "\n",
    "In this tutorial, you'll build a simple Q&A system, that can give semantic answers to questions. Three sample documents from the Azure documentation are provided. Fill out the missing pieces in the source source to get everything working (indicated by `#FIXME`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (1.59.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from openai) (2.10.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tiktoken\n",
    "import openai\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from openai.embeddings_utils import cosine_similarity\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Azure OpenAI Service API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define embedding model and encoding\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "EMBEDDING_ENCODING = 'cl100k_base'\n",
    "EMBEDDING_CHUNK_SIZE = 8000\n",
    "COMPLETION_MODEL = 'text-davinci-003'\n",
    "\n",
    "# initialize tiktoken for encoding text\n",
    "encoding = tiktoken.get_encoding(EMBEDDING_ENCODING)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's read the documents in `/data/qna/*.txt`, which are our sample documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in the samples directory\n",
    "samples_dir = os.path.join(os.getcwd(), \"./data/qna/\")\n",
    "sample_files = os.listdir(samples_dir)\n",
    "\n",
    "# read each file and remove and newlines (better for embeddings later)\n",
    "documents = []\n",
    "for file in sample_files:\n",
    "    with open(os.path.join(samples_dir, file), \"r\") as f:\n",
    "        content = f.read()\n",
    "        content = content.replace(\"\\n\", \" \")\n",
    "        content = content.replace(\"  \", \" \")\n",
    "        documents.append(content)\n",
    "\n",
    "# print some stats about the documents\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "for doc in documents:\n",
    "    num_tokens = len(encoding.encode(doc))\n",
    "    print(f\"Content: {doc[:80]}... \\n---> Tokens: {num_tokens}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all documents loaded, we can embed them using our embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for all docs\n",
    "embeddings = #FIXME\n",
    "\n",
    "# print some stats about the embeddings\n",
    "for e in embeddings:\n",
    "    print(len(e))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our embeddings, we can try to ask some questions and see if it retrieves the correct document. You can try the following questions:\n",
    "\n",
    "* what is azure openai service?\n",
    "* can translator be fine tuned?\n",
    "* what is the difference between luis and clu?\n",
    "* what is form recognizer? (should yield no result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding for question\n",
    "question = \"what is azure openai service?\"\n",
    "qe = #FIXME\n",
    "\n",
    "# calculate cosine similarity between question and each document\n",
    "similaries = #FIXME\n",
    "\n",
    "# Get the matching document, in this case we just use argmax of similarities\n",
    "max_i = #FIXME\n",
    "\n",
    "# print some stats about the similarities\n",
    "for i, s in enumerate(similaries):\n",
    "    print(f\"Similarity to {sample_files[i]} is {s}\")\n",
    "print(f\"Matching document is {sample_files[max_i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1899838685.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    prompt = #FIXME\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Generate a prompt that we use for completion, in this case we put the matched document and the question in the prompt\n",
    "prompt = #FIXME\n",
    "\n",
    "# get response from completion model\n",
    "response = #FIXME\n",
    "answer = #FIXME\n",
    "\n",
    "# print the question and answer\n",
    "print(f\"Question was: {question}\\nRetrieved answer was: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "import openai\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_not_exception_type\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "EMBEDDING_CTX_LENGTH = 8191\n",
    "EMBEDDING_ENCODING = 'cl100k_base'\n",
    "\n",
    "# let's make sure to not retry on an invalid request, because that is what we want to demonstrate\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6), retry=retry_if_not_exception_type(openai.BadRequestError))\n",
    "def get_embedding(text_or_tokens, model=EMBEDDING_MODEL):\n",
    "    return client.embeddings.create(input=text_or_tokens, model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    " \n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "      api_version=\"2024-02-01\",\n",
    "     azure_endpoint=\"https://ia242-m5z9tor6-swedencentral.openai.azure.com/\"\n",
    ")\n",
    "# This will correspond to the custom name you chose for your deployment when you deployed a model.\n",
    "# Use a gpt-35-turbo-instruct deployment.\n",
    "deployment_name = \"davinci-002\"\n",
    " \n",
    "# Send a completion call to generate an answer\n",
    "prompt = \"\"\n",
    "response = client.completions.create(\n",
    "   model=deployment_name,\n",
    "   prompt=prompt,\n",
    "   temperature=1,\n",
    "   max_tokens=100,\n",
    "   top_p=0.5,\n",
    "    frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "     best_of=1,\n",
    "      stop=None\n",
    "  )\n",
    " \n",
    "print(prompt + response.choices[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that worked. Now we should have a simple understanding how Q&A can work using Azure OpenAI Service embeddings and completions. Next step would be:\n",
    "\n",
    "* Chunking of longer documents (you might run into token limits for embeddings and the answering prompt)\n",
    "* Usage of a vector database (pinecone, redis, etc.) to scale the search part to a larger amount of documents\n",
    "* Evaluation of the top k results, instead of just the best matching document\n",
    "* ...and a few more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
