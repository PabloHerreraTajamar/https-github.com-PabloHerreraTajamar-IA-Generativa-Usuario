{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure chat completions example\n",
    "\n",
    "This example will cover chat completions using the Azure OpenAI service. It also includes information on content filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we install the necessary dependencies and import the libraries we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai<2.0.0,>=1.0.0\n",
      "  Using cached openai-1.59.7-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.0.0)\n",
      "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.0.0)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.0.0)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.0.0)\n",
      "  Using cached jiter-0.8.2-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai<2.0.0,>=1.0.0)\n",
      "  Using cached pydantic-2.10.5-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting sniffio (from openai<2.0.0,>=1.0.0)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.0.0)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai<2.0.0,>=1.0.0)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai<2.0.0,>=1.0.0)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai<2.0.0,>=1.0.0)\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.0.0)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.0.0)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai<2.0.0,>=1.0.0)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai<2.0.0,>=1.0.0)\n",
      "  Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.0.0) (0.4.6)\n",
      "Using cached openai-1.59.7-py3-none-any.whl (454 kB)\n",
      "Using cached anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached jiter-0.8.2-cp312-cp312-win_amd64.whl (204 kB)\n",
      "Using cached pydantic-2.10.5-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: typing-extensions, tqdm, sniffio, jiter, idna, h11, distro, certifi, annotated-types, pydantic-core, httpcore, anyio, pydantic, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.8.0 certifi-2024.12.14 distro-1.9.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jiter-0.8.2 openai-1.59.7 pydantic-2.10.5 pydantic-core-2.27.2 sniffio-1.3.1 tqdm-4.67.1 typing-extensions-4.12.2\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install \"openai>=1.0.0,<2.0.0\"\n",
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "\n",
    "The Azure OpenAI service supports multiple authentication mechanisms that include API keys and Azure Active Directory token credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_azure_active_directory = False  # Set this flag to True if you are using Azure Active Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authentication using API key\n",
    "\n",
    "To set up the OpenAI SDK to use an *Azure API Key*, we need to set `api_key` to a key associated with your endpoint (you can find this key in *\"Keys and Endpoints\"* under *\"Resource Management\"* in the [Azure Portal](https://portal.azure.com)). You'll also find the endpoint for your resource here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "if not use_azure_active_directory:\n",
    "    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_version=\"2024-02-01\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authentication using Azure Active Directory\n",
    "Let's now see how we can autheticate via Azure Active Directory. We'll start by installing the `azure-identity` library. This library will provide the token credentials we need to authenticate and help us build a token credential provider through the `get_bearer_token_provider` helper function. It's recommended to use `get_bearer_token_provider` over providing a static token to `AzureOpenAI` because this API will automatically cache and refresh tokens for you. \n",
    "\n",
    "For more information on how to set up Azure Active Directory authentication with Azure OpenAI, see the [documentation](https://learn.microsoft.com/azure/ai-services/openai/how-to/managed-identity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-identity>=1.15.0\n",
      "  Downloading azure_identity-1.19.0-py3-none-any.whl.metadata (80 kB)\n",
      "Collecting azure-core>=1.31.0 (from azure-identity>=1.15.0)\n",
      "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting cryptography>=2.5 (from azure-identity>=1.15.0)\n",
      "  Downloading cryptography-44.0.0-cp39-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting msal>=1.30.0 (from azure-identity>=1.15.0)\n",
      "  Downloading msal-1.31.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity>=1.15.0)\n",
      "  Downloading msal_extensions-1.2.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from azure-identity>=1.15.0) (4.12.2)\n",
      "Collecting requests>=2.21.0 (from azure-core>=1.31.0->azure-identity>=1.15.0)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from azure-core>=1.31.0->azure-identity>=1.15.0) (1.17.0)\n",
      "Collecting cffi>=1.12 (from cryptography>=2.5->azure-identity>=1.15.0)\n",
      "  Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity>=1.15.0)\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting portalocker<3,>=1.4 (from msal-extensions>=1.2.0->azure-identity>=1.15.0)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=2.5->azure-identity>=1.15.0)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from portalocker<3,>=1.4->msal-extensions>=1.2.0->azure-identity>=1.15.0) (308)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.15.0)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.15.0) (3.10)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.15.0)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alumno_ai\\desktop\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.15.0) (2024.12.14)\n",
      "Downloading azure_identity-1.19.0-py3-none-any.whl (187 kB)\n",
      "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
      "Downloading cryptography-44.0.0-cp39-abi3-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.2/3.2 MB 46.7 MB/s eta 0:00:00\n",
      "Downloading msal-1.31.1-py3-none-any.whl (113 kB)\n",
      "Downloading msal_extensions-1.2.0-py3-none-any.whl (19 kB)\n",
      "Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: urllib3, PyJWT, pycparser, portalocker, charset-normalizer, requests, cffi, cryptography, azure-core, msal, msal-extensions, azure-identity\n",
      "Successfully installed PyJWT-2.10.1 azure-core-1.32.0 azure-identity-1.19.0 cffi-1.17.1 charset-normalizer-3.4.1 cryptography-44.0.0 msal-1.31.1 msal-extensions-1.2.0 portalocker-2.10.1 pycparser-2.22 requests-2.32.3 urllib3-2.3.0\n"
     ]
    }
   ],
   "source": [
    "! pip install \"azure-identity>=1.15.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "if use_azure_active_directory:\n",
    "    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "\n",
    "    client = openai.AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"),\n",
    "        api_version=\"2023-09-01-preview\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: the AzureOpenAI infers the following arguments from their corresponding environment variables if they are not provided:\n",
    "\n",
    "- `api_key` from `AZURE_OPENAI_API_KEY`\n",
    "- `azure_ad_token` from `AZURE_OPENAI_AD_TOKEN`\n",
    "- `api_version` from `OPENAI_API_VERSION`\n",
    "- `azure_endpoint` from `AZURE_OPENAI_ENDPOINT`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployments\n",
    "\n",
    "In this section we are going to create a deployment of a GPT model that we can use to create chat completions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployments: Create in the Azure OpenAI Studio\n",
    "Let's deploy a model to use with chat completions. Go to https://portal.azure.com, find your Azure OpenAI resource, and then navigate to the Azure OpenAI Studio. Click on the \"Deployments\" tab and then create a deployment for the model you want to use for chat completions. The deployment name that you give the model will be used in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = \"gpt-4o-mini\" # Fill in the deployment name from the portal here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chat completions\n",
    "\n",
    "Now let's create a chat completion using the client we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Hoy es 27 de octubre de 2023. Si necesitas información adicional sobre esta fecha o algún evento específico, no dudes en preguntar.\n"
     ]
    }
   ],
   "source": [
    "# For all possible arguments see https://platform.openai.com/docs/api-reference/chat-completions/create\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explicame que dia es hoy\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].message.role}: {response.choices[0].message.content}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a streaming chat completion\n",
    "\n",
    "We can also stream the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: ¡Hola! ¿Quién está ahí?"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Eres un asistente útil.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Toc, toc.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "\n",
    "        if delta.role:\n",
    "            print(delta.role + \": \", end=\"\", flush=True)\n",
    "        if delta.content:\n",
    "            print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content filtering\n",
    "\n",
    "Azure OpenAI service includes content filtering of prompts and completion responses. You can learn more about content filtering and how to configure it [here](https://learn.microsoft.com/azure/ai-services/openai/concepts/content-filter).\n",
    "\n",
    "If the prompt is flagged by the content filter, the library will raise a `BadRequestError` exception with a `content_filter` error code. Otherwise, you can access the `prompt_filter_results` and `content_filter_results` on the response to see the results of the content filtering and what categories were flagged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt flagged by content filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Eres un asistente útil.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Toc, toc.\"},\n",
    "    ]\n",
    "\n",
    "try:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=messages\n",
    "    )\n",
    "except openai.BadRequestError as e:\n",
    "    err = json.loads(e.response.text)\n",
    "    if err[\"error\"][\"code\"] == \"content_filter\":\n",
    "        print(\"Content filter triggered!\")\n",
    "        content_filter_result = err[\"error\"][\"innererror\"][\"content_filter_result\"]\n",
    "        for category, details in content_filter_result.items():\n",
    "            print(f\"{category}:\\n filtered={details['filtered']}\\n severity={details['severity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the result of the content filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The biggest city in Washington state is Seattle. It is the largest city in terms of population and is well known for its cultural, economic, and technological impact.\n",
      "\n",
      "Prompt content filter results:\n",
      "hate:\n",
      " filtered=False\n",
      " severity=safe\n",
      "jailbreak:\n",
      " filtered=False\n",
      " severity=N/A\n",
      "self_harm:\n",
      " filtered=False\n",
      " severity=safe\n",
      "sexual:\n",
      " filtered=False\n",
      " severity=safe\n",
      "violence:\n",
      " filtered=False\n",
      " severity=safe\n",
      "\n",
      "Completion content filter results:\n",
      "hate:\n",
      " filtered=False\n",
      " severity=safe\n",
      "self_harm:\n",
      " filtered=False\n",
      " severity=safe\n",
      "sexual:\n",
      " filtered=False\n",
      " severity=safe\n",
      "violence:\n",
      " filtered=False\n",
      " severity=safe\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the biggest city in Washington?\"}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "# Verificación de la respuesta para obtener la respuesta generada\n",
    "if len(completion.choices) > 0 and hasattr(completion.choices[0], 'message'):\n",
    "    print(f\"Answer: {completion.choices[0].message.content}\")\n",
    "else:\n",
    "    print(\"No answer returned or message content not found.\")\n",
    "\n",
    "# Verificación del resultado del filtro de contenido de la solicitud del modelo\n",
    "if hasattr(completion, 'model_extra') and \"prompt_filter_results\" in completion.model_extra:\n",
    "    prompt_filter_result = completion.model_extra[\"prompt_filter_results\"][0].get(\"content_filter_results\", {})\n",
    "    print(\"\\nPrompt content filter results:\")\n",
    "    if prompt_filter_result:\n",
    "        for category, details in prompt_filter_result.items():\n",
    "            # Verifica si 'filtered' y 'severity' existen antes de acceder\n",
    "            filtered = details.get('filtered', 'N/A')  # Default to 'N/A' if not found\n",
    "            severity = details.get('severity', 'N/A')  # Default to 'N/A' if not found\n",
    "            print(f\"{category}:\\n filtered={filtered}\\n severity={severity}\")\n",
    "    else:\n",
    "        print(\"No content filter results found in prompt filter.\")\n",
    "\n",
    "# Verificación del resultado del filtro de contenido de la respuesta del modelo\n",
    "if len(completion.choices) > 0 and hasattr(completion.choices[0], 'model_extra') and \"content_filter_results\" in completion.choices[0].model_extra:\n",
    "    completion_filter_result = completion.choices[0].model_extra[\"content_filter_results\"]\n",
    "    print(\"\\nCompletion content filter results:\")\n",
    "    if completion_filter_result:\n",
    "        for category, details in completion_filter_result.items():\n",
    "            # Verifica si 'filtered' y 'severity' existen antes de acceder\n",
    "            filtered = details.get('filtered', 'N/A')  # Default to 'N/A' if not found\n",
    "            severity = details.get('severity', 'N/A')  # Default to 'N/A' if not found\n",
    "            print(f\"{category}:\\n filtered={filtered}\\n severity={severity}\")\n",
    "    else:\n",
    "        print(\"No content filter results found in completion.\")\n",
    "else:\n",
    "    print(\"No content filter results found in completion response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1: Implementar streaming con diferentes temperaturas\n",
    "# Instrucciones: Usa el siguiente código para generar respuestas a través de streaming. Cambia el valor de 'temperature' a 0.9 y 0.3 y compara los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumen de los efectos de la temperature:\n",
    "- Baja temperatura (0-0.3): Respuestas más predecibles, coherentes y repetitivas.\n",
    "- Temperatura moderada (0.5-0.7): Equilibrio entre coherencia y creatividad.\n",
    "- Alta temperatura (0.8-1.5): Respuestas más creativas, diversas, pero menos coherentes y predecibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: La inteligencia artificial (IA) es un campo de la informática que se centra en la creación de sistemas y programas capaces de realizar tareas que normalmente requieren inteligencia humana. Esto incluye habilidades como el razonamiento, la resolución de problemas, el aprendizaje, la percepción y el lenguaje.\n",
      "\n",
      "Existen varios tipos de inteligencia artificial, que se pueden clasificar en dos grandes categorías:\n",
      "\n",
      "1. **IA débil**: También conocida como IA estrecha, está diseñada y entrenada para realizar tareas específicas. Ejemplos de IA débil incluyen asistentes virtuales como Siri y Alexa, sistemas de recomendación en plataformas de streaming o comercio electrónico, y software de reconocimiento de imágenes. Aunque estos sistemas pueden parecer inteligentes, no poseen comprensión o conciencia real.\n",
      "\n",
      "2. **IA fuerte**: Esta categoría se refiere a un tipo de IA que tendría una comprensión y razonamiento humano completos. La IA fuerte sería capaz de realizar cualquier tarea intelectual que un ser humano podría realizar. En la actualidad, este tipo de IA aún no existe y es un tema de debate tanto en la ciencia como en la filosofía.\n",
      "\n",
      "La IA utiliza diversas técnicas y enfoques, entre los que se incluyen:\n",
      "\n",
      "- **Aprendizaje automático (Machine Learning)**: Un subcampo de la IA que permite a las máquinas aprender de los datos. Utiliza algoritmos que identifican patrones en grandes conjuntos de datos y mejoran su desempeño con el tiempo.\n",
      "\n",
      "- **Redes neuronales**: Inspiradas en el funcionamiento del cerebro humano, las redes neuronales son un conjunto de algoritmos que intentan reconocer patrones. Se utilizan en aplicaciones complejas como el reconocimiento de voz, la visión por computadora y el procesamiento del lenguaje natural.\n",
      "\n",
      "- **Procesamiento del lenguaje natural (NLP)**: Permite a las máquinas entender, interpretar y generar lenguaje humano en una forma que tiene sentido. Esto se aplica en chatbots, traductores automáticos y asistentes virtuales.\n",
      "\n",
      "- **Visión por computadora**: Esta área se ocupa de cómo las computadoras pueden ser programadas para entender imágenes y vídeos. Se utiliza en aplicaciones de reconocimiento facial, análisis de imágenes médicas y vehículos autónomos.\n",
      "\n",
      "La inteligencia artificial tiene un amplio rango de aplicaciones en diferentes sectores, como la medicina, la educación, la industria, la agricultura y el entretenimiento. Sin embargo, también plantea desafíos éticos y sociales, como la privacidad de los datos, el sesgo en los algoritmos y el impacto en el empleo.\n",
      "\n",
      "En resumen, la inteligencia artificial es una área fascinante y en rápida evolución que tiene el potencial de transformar numerosos aspectos de nuestra vida cotidiana y la forma en que interactuamos con la tecnología."
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hazme una explicacion sobre la inteligencia artificial\"},\n",
    "    ],\n",
    "    temperature=0.9,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "        if delta.role:\n",
    "            print(delta.role + \": \", end=\"\", flush=True)\n",
    "        if delta.content:\n",
    "            print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: La inteligencia artificial (IA) es un campo de la informática que se centra en la creación de sistemas capaces de realizar tareas que normalmente requieren inteligencia humana. Estas tareas incluyen el aprendizaje, el razonamiento, la percepción, la comprensión del lenguaje natural y la toma de decisiones. La IA se basa en algoritmos y modelos matemáticos que permiten a las máquinas procesar información, identificar patrones y hacer predicciones.\n",
      "\n",
      "### Tipos de Inteligencia Artificial\n",
      "\n",
      "1. **IA Débil (o Narrow AI)**: Se refiere a sistemas diseñados para realizar tareas específicas. Por ejemplo, asistentes virtuales como Siri o Alexa, sistemas de recomendación en plataformas de streaming, o programas de reconocimiento de imágenes. Aunque son muy eficaces en sus áreas, no poseen una comprensión general del mundo.\n",
      "\n",
      "2. **IA Fuerte (o General AI)**: Este es un concepto más teórico que se refiere a una inteligencia que puede entender, aprender y aplicar conocimientos de manera similar a un ser humano en una variedad de contextos. Actualmente, no existe una IA fuerte.\n",
      "\n",
      "### Técnicas y Enfoques\n",
      "\n",
      "- **Aprendizaje Automático (Machine Learning)**: Es una subdisciplina de la IA que permite a las máquinas aprender de datos y mejorar su rendimiento con el tiempo sin ser programadas explícitamente para cada tarea. Incluye técnicas como el aprendizaje supervisado, no supervisado y por refuerzo.\n",
      "\n",
      "- **Redes Neuronales**: Inspiradas en el funcionamiento del cerebro humano, estas estructuras son fundamentales en el aprendizaje profundo (Deep Learning), una técnica que ha demostrado ser muy efectiva en tareas complejas como el reconocimiento de voz y la visión por computadora.\n",
      "\n",
      "- **Procesamiento del Lenguaje Natural (NLP)**: Esta área se ocupa de la interacción entre las computadoras y el lenguaje humano. Permite a las máquinas entender, interpretar y generar texto en lenguaje natural, lo que es esencial para aplicaciones como chatbots y traductores automáticos.\n",
      "\n",
      "### Aplicaciones de la IA\n",
      "\n",
      "La inteligencia artificial tiene una amplia gama de aplicaciones en diversos sectores:\n",
      "\n",
      "- **Salud**: Diagnóstico de enfermedades, análisis de imágenes médicas y personalización de tratamientos.\n",
      "- **Finanzas**: Detección de fraudes, análisis de riesgos y asesoría financiera automatizada.\n",
      "- **Transporte**: Vehículos autónomos y optimización de rutas de entrega.\n",
      "- **Marketing**: Segmentación de clientes, análisis de tendencias y personalización de ofertas.\n",
      "- **Manufactura**: Automatización de procesos y mantenimiento predictivo.\n",
      "\n",
      "### Desafíos y Consideraciones Éticas\n",
      "\n",
      "A pesar de sus beneficios, la IA también plantea desafíos y preocupaciones éticas, como:\n",
      "\n",
      "- **Sesgo Algorítmico**: Los sistemas de IA pueden perpetuar o amplificar sesgos existentes en los datos con los que fueron entrenados.\n",
      "- **Privacidad**: La recopilación y el uso de datos personales pueden comprometer la privacidad de los individuos.\n",
      "- **Desempleo**: La automatización de tareas puede llevar a la pérdida de empleos en ciertos sectores.\n",
      "- **Responsabilidad**: Determinar quién es responsable de las decisiones tomadas por sistemas de IA puede ser complicado.\n",
      "\n",
      "En resumen, la inteligencia artificial es una herramienta poderosa con el potencial de transformar muchas áreas de nuestra vida, pero también requiere un enfoque cuidadoso y ético para asegurar que se utilice de manera responsable y beneficiosa para la sociedad."
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hazme una explicacion sobre la inteligencia artificial\"},\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "        if delta.role:\n",
    "            print(delta.role + \": \", end=\"\", flush=True)\n",
    "        if delta.content:\n",
    "            print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba con diferentes valores de temperatura y analiza cómo cambia la respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2: Detectar y manejar errores de filtrado de contenido\n",
    " Instrucciones: Intenta enviar un mensaje que probablemente sea filtrado. Implementa un manejo de excepciones adecuado para alertar al usuario y sugerir un nuevo mensaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "¡Filtro de contenido activado! Por favor, reformula tu mensaje.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Eres un asistente útil.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Necesito instrucciones para hackear la cuenta de alguien.\"},\n",
    "]\n",
    "\n",
    "try:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=messages\n",
    "    )\n",
    "except openai.BadRequestError as e:\n",
    "    err = json.loads(e.response.text)  # Decodificamos el error de la respuesta\n",
    "    print(err)\n",
    "    if err[\"error\"][\"code\"] == \"content_filter\":  # Si el error es por filtro de contenido\n",
    "        print(\"¡Filtro de contenido activado! Por favor, reformula tu mensaje.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3: Inspeccionar y analizar los resultados del filtro de contenido\n",
    "Instrucciones: Envía una pregunta general y analiza los resultados del filtro para diferentes categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content filter triggered!\n",
      "hate:\n",
      " filtered=False\n",
      " \n",
      "jailbreak:\n",
      " filtered=False\n",
      " \n",
      "self_harm:\n",
      " filtered=False\n",
      " \n",
      "sexual:\n",
      " filtered=False\n",
      " \n",
      "violence:\n",
      " filtered=True\n",
      " \n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Eres un asistente útil.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Necesito instrucciones para hackear el telefono de alguien.\"},\n",
    "]\n",
    "\n",
    "try:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=messages\n",
    "    )\n",
    "except openai.BadRequestError as e:\n",
    "    err = json.loads(e.response.text)\n",
    "    if err[\"error\"][\"code\"] == \"content_filter\":\n",
    "        print(\"Content filter triggered!\")\n",
    "        content_filter_result = err[\"error\"][\"innererror\"][\"content_filter_result\"]\n",
    "        for category, details in content_filter_result.items():\n",
    "            print(f\"{category}:\\n filtered={details['filtered']}\\n \")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4: Crear una función que automatice la verificación de contenido filtrado\n",
    "# Instrucciones: Crea una función que reciba un mensaje y devuelva si fue filtrado o no, y en qué categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content filter triggered!\n",
      "hate:\n",
      " filtered=False\n",
      " severity=safe\n",
      "jailbreak:\n",
      " filtered=False\n",
      " severity=N/A\n",
      "self_harm:\n",
      " filtered=False\n",
      " severity=safe\n",
      "sexual:\n",
      " filtered=False\n",
      " severity=safe\n",
      "violence:\n",
      " filtered=True\n",
      " severity=medium\n"
     ]
    }
   ],
   "source": [
    "def check_content_filter(messages):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Eres un asistente útil.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Necesito instrucciones para hackear el telefono de alguien.\"},\n",
    "]\n",
    "    try: \n",
    "        completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=messages\n",
    "    )\n",
    "        print(\"Message passed the filter.\")\n",
    "    except openai.BadRequestError as e:\n",
    "        err = json.loads(e.response.text)\n",
    "        if err[\"error\"][\"code\"] == \"content_filter\":\n",
    "            print(\"Content filter triggered!\")\n",
    "            content_filter_result = err[\"error\"][\"innererror\"][\"content_filter_result\"]\n",
    "            for category, details in content_filter_result.items():\n",
    "                print(f\"{category}:\\n filtered={details['filtered']}\\n severity={details.get('severity', 'N/A')}\")\n",
    "\n",
    "check_content_filter(\"Describe how to make a bomb.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
